# -*- coding: utf-8 -*-
"""Chennai House Price Predication.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EfDybJYdBkmvAUbnbM144ptRDlkLnupb

**Importing the Libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

import warnings
warnings.filterwarnings("ignore")

"""**Loading the Csv File**

"""

df = pd.read_csv('/content/train-chennai-sale.csv')
df

"""**Dropping columns which is not neccesary**"""

df = df.drop(['PRT_ID','REG_FEE','COMMIS'],axis=1)

df.head(5)

"""**Handling missing Values**"""

# Checking for Null Values
df.isnull().sum()

"""**Filling the Null Values**"""

df['N_BEDROOM'].value_counts()

df['N_BEDROOM'].fillna(df['N_BEDROOM'].mode()[0],inplace=True)

df['N_BATHROOM'].value_counts()

df['N_BATHROOM'].fillna(df['N_BATHROOM'].mode()[0],inplace=True)

df['QS_OVERALL'].value_counts()

df['QS_OVERALL'].fillna(df['QS_OVERALL'].median(),inplace=True)

df.isnull().sum()

"""**Droping the duplicates**"""

df = df.drop_duplicates()

df.head(2)

"""**Checking Datatypes**"""

df.dtypes

"""**Fixing the Datatype in correct Manner**"""

df.N_BEDROOM = df.N_BEDROOM.astype(int)
df.N_BATHROOM = df.N_BATHROOM.astype(int)
df.DATE_SALE = pd.to_datetime(df.DATE_SALE, format='%d-%m-%Y')
df.DATE_BUILD = pd.to_datetime(df.DATE_BUILD, format='%d-%m-%Y')

"""**Creating AGE column which can deternine how old the property** **is** """

df['AGE'] = pd.DatetimeIndex(df['DATE_SALE']).year - pd.DatetimeIndex(df['DATE_BUILD']).year

df.head(10)

df.describe()

"""**Checking for Outliers**"""

sns.boxplot(df['SALES_PRICE'])

"""There is No need to correct outliers i.e House sales will be more than 2 crore

**Interquartile Range**
"""

IQR25 = df['SALES_PRICE'].quantile(0.25)
IQR75 = df['SALES_PRICE'].quantile(0.75)
print(IQR75-IQR25)

"""**Finding MAX & MIN Values**"""

max = IQR75 + 1.5 * (IQR75-IQR25)
min = IQR25 - 1.5 * (IQR75-IQR25)
print(max)
print(min)

"""**Correcting the Categorical columns**"""

# checking unique elements in AREA
df.AREA.unique()

# correcting the spelling mistakes in AREA
df.AREA.replace(['Karapakkam'],'Karapakam',inplace=True)
df.AREA.replace(['Ana Nagar','Ann Nagar'],'Anna Nagar',inplace=True)
df.AREA.replace(['Adyr'],'Adyar',inplace=True)
df.AREA.replace(['Velchery'],'Velachery',inplace=True)
df.AREA.replace(['Chrompet','Chrompt', 'Chrmpet'],'Chormpet',inplace=True)
df.AREA.replace(['KKNagar'],'KK Nagar',inplace=True)
df.AREA.replace(['TNagar'],'T Nagar',inplace=True)

df.AREA.unique()

# checking unique elements in SALE_COND
df.SALE_COND.unique()

# correcting the spelling mistakes in SALE_COND
df.SALE_COND.replace(['Adj Land'],'AdjLand',inplace=True)
df.SALE_COND.replace(['Ab Normal'],'AbNormal',inplace=True)
df.SALE_COND.replace(['Partiall','PartiaLl'],'Partial',inplace=True)

df.SALE_COND.unique()

# checking unique elements in PARK_FACIL
df.PARK_FACIL.unique()

# correcting the spelling mistakes in PARK_FACIL
df.PARK_FACIL.replace(['Noo'],'No',inplace=True)

df.PARK_FACIL.unique()

# checking unique elements in BUILDTYPE
df.BUILDTYPE.unique()

df.BUILDTYPE.replace(['Others'],'Other',inplace=True)
df.BUILDTYPE.replace(['Comercial'],'Commercial',inplace=True)

df.BUILDTYPE.unique()

# checking unique elements in UTILITY_AVAIL
df.UTILITY_AVAIL.unique()

# correcting the spelling mistakes in UTILITY_AVAIL
df.UTILITY_AVAIL.replace(['All Pub'],'AllPub',inplace=True)
df.UTILITY_AVAIL.replace(['NoSewr '],'NoSeWa',inplace=True)

df.UTILITY_AVAIL.unique()

# checking unique elements in STREET
df.STREET.unique()

# correcting the spelling mistakes in STREET
df.STREET.replace(['Pavd'],'Paved',inplace=True)
df.STREET.replace(['NoAccess'], 'No Access',inplace=True)

df.STREET.unique()

# checking unique elements in MZZONE
df.MZZONE.unique()

"""**EXPLORATORY DATA ANALYSIS**"""

plt.figure(figsize=(15,10))
sns.heatmap(df.corr(method='pearson'), cbar=False, annot=True, fmt='.1f', linewidth=0.2, cmap='coolwarm');

"""We can see that in numerical columns there are some correlation between target and features

**Catagorical columns**
"""

# Checking Realtionship between Area and Sales price
area_order = df.groupby('AREA')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['AREA']

df.groupby('AREA')['SALES_PRICE'].mean().plot()
plt.title('AREA vs SALES_PRICE')
plt.xlabel('AREA')
plt.ylabel('SALES_PRICE')
plt.show()

sns.barplot(x='AREA',y='SALES_PRICE',data=df,order = area_order)

"""1. There is a Linear relationship between Area and Sales price will go for label 
encoding
2. T Nagar has the highest sales in terms of Area
"""

# Checking Realtionship between SALE_COND and Sales price
sale_cond_order = df.groupby('SALE_COND')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['SALE_COND']

df.groupby('SALE_COND')['SALES_PRICE'].mean().plot()
plt.title('SALE_COND vs SALES_PRICE')
plt.xlabel('SALE_COND')
plt.ylabel('SALES_PRICE')
plt.show()

sns.barplot(x='SALE_COND',y='SALES_PRICE',data=df,order=sale_cond_order)

"""There is no Linear relationship between Sales_cond and Sales price will drop this feature"""

# Checking Realtionship between PARK_FACIL and Sales price
df.groupby('PARK_FACIL')['SALES_PRICE'].mean().plot()
plt.title('PARK_FACIL vs SALES_PRICE')
plt.xlabel('PARK_FACIL')
plt.ylabel('SALES_PRICE')
plt.show()

sns.barplot(x='PARK_FACIL',y='SALES_PRICE',data=df)

"""1. There is a Linear relationship between PARK_FACIL and Sales price will go for label encoding
2. With Parking facilty is more costlier compared to No parking facility
"""

# Checking Realtionship between BUILDTYPE and Sales price
df.groupby('BUILDTYPE')['SALES_PRICE'].mean().plot()
plt.title('BUILDTYPE vs SALES_PRICE')
plt.xlabel('BUILDTYPE')
plt.ylabel('SALES_PRICE')
plt.show()

sns.barplot(x='BUILDTYPE',y='SALES_PRICE',data=df)

"""1. There is a no linear relationship between BUILDTYPE and Sales price will go for One Hot encoding
2. Commercial buildings are Costlier.
"""

# Checking Realtionship between UTILITY_AVAIL and Sales price
Utility_avail_order = df.groupby('UTILITY_AVAIL')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['UTILITY_AVAIL']

df.groupby('UTILITY_AVAIL')['SALES_PRICE'].mean().plot()
plt.title('UTILITY_AVAIL vs SALES_PRICE')
plt.xlabel('UTILITY_AVAIL')
plt.ylabel('SALES_PRICE')
plt.show()

sns.barplot(x='UTILITY_AVAIL',y='SALES_PRICE',data=df,order=Utility_avail_order)

"""1. There is a Linear relationship between UTILITY_AVAIL and Sales price will go for label encoding
2. All Pub are Costlier
"""

# Checking Realtionship between STREET and Sales price
Street_order = df.groupby('STREET')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['STREET']

df.groupby('STREET')['SALES_PRICE'].mean().plot()
plt.title('STREET vs SALES_PRICE')
plt.xlabel('STREET')
plt.ylabel('SALES_PRICE')
plt.show()

sns.barplot(x='STREET',y='SALES_PRICE',data=df,order=Street_order)

"""1. There is a Linear relationship between Street and Sales price will go for label encoding
2. With Gravel street are costlier
"""

# Checking Realtionship between MZZONE and Sales price
Mzzone_order = df.groupby('MZZONE')['SALES_PRICE'].mean().reset_index().sort_values('SALES_PRICE')['MZZONE']

df.groupby('MZZONE')['SALES_PRICE'].mean().plot()
plt.title('MZZONE vs SALES_PRICE')
plt.xlabel('MZZONE')
plt.ylabel('SALES_PRICE')
plt.show()

sns.barplot(x='MZZONE',y='SALES_PRICE',data=df,order=Mzzone_order)

"""1. There is a Linear relationship between MZZONE and Sales price will go for label encoding
2. With MZZONE 'RM' is costlier than others

**Numerical columns**
"""

# Checking Realtionship between N_BEDROOM  and Sales price

sns.barplot(x='N_BEDROOM',y='SALES_PRICE',data=df)

"""We found a relation so this feature will be important"""

# Checking Realtionship between N_BATHROOM and Sales price

sns.barplot(x='N_BATHROOM',y='SALES_PRICE',data=df)

"""We found a relation so this feature will be important"""

# Checking Realtionship between N_ROOM and Sales price
sns.barplot(x='N_ROOM',y='SALES_PRICE',data=df)

"""We found a relation so this feature will be important"""

sns.distplot(df['QS_ROOMS'])

sns.regplot(df.QS_ROOMS, df.SALES_PRICE, scatter_kws={"color": "red"}, line_kws={"color": "black"})

"""There is no relation so this feature will be Dropped"""

sns.distplot(df['QS_BEDROOM'])

sns.regplot(df.QS_BEDROOM, df.SALES_PRICE, scatter_kws={"color": "red"}, line_kws={"color": "black"})

"""There is no relation so this feature will be Dropped"""

sns.distplot(df['QS_BATHROOM'])

sns.regplot(df.QS_BATHROOM, df.SALES_PRICE, scatter_kws={"color": "red"}, line_kws={"color": "black"})

"""There is no relation so this feature will be Dropped"""

sns.distplot(df['QS_OVERALL'])

sns.regplot(df.QS_OVERALL, df.SALES_PRICE, scatter_kws={"color": "red"}, line_kws={"color": "black"})

"""There is no relation so this feature will be Dropped"""

sns.distplot(df['INT_SQFT'])

sns.regplot(df.INT_SQFT, df.SALES_PRICE, scatter_kws={"color": "red"}, line_kws={"color": "black"})

"""We found a relation so this feature will be important"""

sns.distplot(df['DIST_MAINROAD'])

sns.regplot(df.DIST_MAINROAD, df.SALES_PRICE, scatter_kws={"color": "red"}, line_kws={"color": "black"})

"""There is no relation so this feature will be Dropped"""

sns.distplot(df['AGE'])

sns.regplot(df.AGE, df.SALES_PRICE, scatter_kws={"color": "red"}, line_kws={"color": "black"})

"""We found a relation so this feature will be important

**Removing unnecessary columns for Encoding**
"""

df.drop(['QS_OVERALL','DIST_MAINROAD','QS_ROOMS','QS_BEDROOM','QS_BATHROOM','DATE_BUILD','DATE_SALE','SALE_COND'],axis=1,inplace=True)

df.head(10)

df.describe()

"""**Encoding**"""

# Label Encoding
# Encoding the area column
df.AREA = df.AREA.map({'Karapakam':0,
                       'Adyar':1,
                       'Chormpet':2,
                       'Velachery':3,
                       'KK Nagar':4,
                       'Anna Nagar':5,
                       'T Nagar':6})

# Encoding the park_facil column
df.PARK_FACIL = df.PARK_FACIL.map({'Yes':0,
                                       'No':1})

# Encoding the utility_avail column
df.UTILITY_AVAIL = df.UTILITY_AVAIL.map({'ELO' : 0,
                                         'NoSeWa' : 1,
                                         'AllPub' : 2})

# Encoding the street column
df.STREET = df.STREET.map({'No Access' : 0,
                               'Paved' : 1, 
                               'Gravel' : 2})

# Encoding the mzzone column 
df.MZZONE = df.MZZONE.map({'A' : 0,
                               'C' : 1,
                               'I' : 2,
                               'RH' : 3,
                               'RL' : 4,
                               'RM' : 5})

# one hot encoding for BuildType
df = pd.get_dummies(df,columns=['BUILDTYPE'])

df.head(10)

df.dtypes

"""**SUPERVISED REGRESSION PROBLEM**
We have the target variable "SALES_PRICE" and the type of Target variable is continuous. We are Predicting a continuous value.

**Importing the Libraries**
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression
from sklearn import neighbors
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
from sklearn import metrics
from sklearn.metrics import mean_squared_error 
from sklearn.metrics import r2_score
from sklearn.datasets import make_regression
from sklearn import tree
import xgboost as xg
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler

scaler=StandardScaler()
minmax=MinMaxScaler()

X = df.drop('SALES_PRICE',axis=1)
y = df['SALES_PRICE']

# Splitting
X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.2)

# Scaling
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""**LINEAR REGRESSION**"""

lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)
print('R2- SCORE:', metrics.r2_score(y_test,y_pred))

# With Cross validation
lr_model=LinearRegression()
CV_Linear_regression = cross_val_score(lr_model,X_train_scaled,y_train,cv=10).mean()
print("The cross_validation score of an Linear_Regression model is:",CV_Linear_regression)

"""**KNN**"""

knn = KNeighborsRegressor(n_neighbors=10)
knn.fit(X_train, y_train)
y_predknn = knn.predict(X_test)
print('R2- SCORE:', metrics.r2_score(y_test,y_predknn))

for i in [1,2,3,4,5,6,7,8,9,10]:
  knn = KNeighborsRegressor(i)
  knn.fit(X_train_scaled,y_train)
  print("k value : ", i, "train score : ",knn.score(X_train_scaled,y_train), "score : ",np.mean(cross_val_score(knn,X_train_scaled,y_train,cv=10)))

# With Cross validation
CV_knn = cross_val_score(knn,X_train_scaled,y_train,cv=10).mean()
print("The cross_validation score of an KNN is:",CV_knn)

"""**DECISION TREE**"""

dt = DecisionTreeRegressor()
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)
print('R2- SCORE:', metrics.r2_score(y_test,y_pred_dt))

# With Cross validation
CV_DecissionTree = cross_val_score(dt,X_train_scaled,y_train,cv=10).mean()
print("The cross_validation score of an DecissionTree model is:",CV_DecissionTree)

"""**XGBOOST**"""

# xg_model = XGBRegressor()
xg_model= xg.XGBRegressor(colsample_bytree=0.4,
                 gamma=2,                 
                 learning_rate=0.01,
                 max_depth=4,
                 min_child_weight=1.5,
                 n_estimators=10000,                                                                    
                )
xg_model.fit(X_train_scaled,y_train)
xg_y_pred=xg_model.predict(X_test_scaled)
print("XGBoost model Rsquared metric:",r2_score(y_test,xg_y_pred))

# With Cross validation
CV_XG_Boost = cross_val_score(xg_model,X_train_scaled,y_train,cv=10).mean()
print("The cross_validation score of an XG_Boost model is:",CV_XG_Boost)

"""**RANDOM FOREST**"""

rf = RandomForestRegressor(n_estimators= 1000, max_depth = 4, max_features='sqrt')
rf.fit(X_train, y_train)
rf_pred = rf.predict(X_test)
print("Random forest Regressor model Rsquared metric",r2_score(y_test,rf_pred))

# With Cross validation
CV_RandomForest = cross_val_score(rf,X_train_scaled,y_train,cv=10).mean()
print("The cross_validation score of an XG_Boost model is:",CV_RandomForest)

"""**R_SQUARED_SCORES**

*   The score of an LINEAR REGRESSION model is : **0.9167193473150832**
*   The score of an XGBOOST model is : **0.9958181844706115**
*   The score of an RANDOM FOREST model is : **0.8733452467440321**
*   The score of an KNN model is : **0.44307012317069194**
*   The score of an Decission Tree model is : **0.9730249147855443**

**R_SQUARED_SCORES**

*   The cross_validation score of an LINEAR REGRESSION model is : **0.9199234394602781**
*   The cross_validation score of an XGBOOST model is : **0.99573147754153**
*   The cross_validation score of an RANDOM FOREST model is :  **0.8700539664197182**
*   The cross_validation score of an KNN model is :**0.9505331453378817**
*   The cross_validation score of an Decission Tree model is : **0.9725523672291112**

**BEST MODEL**

* XGBoost model give me the high accuracy
"""